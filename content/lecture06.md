Title:  Lecture 6. Logistic Regression
Date: Monday, September 26, 2016

Learning Objectives:

* Undersand the supervised learning classification problem formulation
* Understand the probabilistic interpretation of logistic regression
    * Know why we use the sigmoid function rather than a hard threshold
    * Write down the likelihood function
    * Be able to take the gradient of the negative log-likelihood objective
* Use Newton's method to find the maximum likelihood parameter estimate
    * Understand why Newton's method applies here
    * Understand the difference between gradient descent and Newton's method
    * Understand Newton's method geometrically for one-dimensional problems
* Know that there is no closed-form solution for logistic regression
* Understand logistic regression as a linear classifier
* Know how logistic regression can be generalized to softmax regression for multiclass problems

Helpful Resources:

* **Murphy, ยง8.1-8.3** Logistic Regression
* **Bishop, ยง4.2**: Probabilistic Generative Models
* **Bishop, ยง4.3**: Probabilistic Discriminative Models
* **CS229, Lecture Notes #1:** [Part II, Classification & Logistic Regression](http://cs229.stanford.edu/notes/cs229-notes1.pdf)
* **CS229, Supplemental Notes #1:** [Binary Classification & Logistic Regression](http://cs229.stanford.edu/extra-notes/loss-functions.pdf)
