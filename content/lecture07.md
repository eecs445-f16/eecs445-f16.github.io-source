Title:  Lecture 7. Naive Bayes
Date: Wednesday, September 28, 2016

Learning Objectives:

* Understand the difference between generative and discriminative classifiers
    * Know which models we've used belong to which category
* Naive Bayes Classifiers
    * Understand the conditional independence assumption and implications
    * Know how to write down the likelihood function
    * Be able to compute MLE/MAP estimates by hand
    * Understand Laplace smoothing and the problem it solves
* Compare Logistic Regression and Naive Bayes

Helpful Resources:

* **Murphy, ยง3.1-3.4:** Generative Models for Discrete Data
* **Murphy, ยง3.5:** Naive Bayes Classifiers
* **Murphy, ยง8.6:** Generative vs. Discriminative Classifiers
* **CS229, Lecture Notes #2:** [Part IV, Generative Learning Algorithms](http://cs229.stanford.edu/notes/cs229-notes2.pdf)

Advanced Reading, only if you're interested:

* **Paper:** Ng & Jordan 2001, [On Discriminative vs. Generative Classifiers:  A Comparison of Logistic Regression and Naive Bayes](http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf)
* **Paper:** Zhang, H., 2004. ["The optimality of naive Bayes"](http://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf). AA, 1(2), p.3.
* **Paper:** Domingos, P. and Pazzani, M., 1997. ["On the optimality of the simple Bayesian classifier under zero-one loss"](http://link.springer.com/article/10.1023/A:1007413511361). Machine learning, 29(2-3), pp.103-130.
