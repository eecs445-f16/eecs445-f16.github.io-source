Title:  Lecture 4. Linear Regression, Part I
Date: Monday, September 19, 2016

Learning Objectives:

* Supervised learning regression problem
* Understand the assumptions made by the model of linear regression
* Understand the difference between input $x$ and features $\phi(x)$
* Intuitively understand the least squares objective function
* Use gradient descent to minimize the objective function
    * Use matrix algebra to compute the gradient of the objective
    * Pros and cons of batch vs. stochastic gradient descent
* Use the normal equations to find the least squares solution
    * Know how to derive the pseudoinverse
    * Understand the normal equations geometrically

Helpful Resources:

* **Murphy, ยง7.1-7.3:** Linear Regression
* **Bishop, ยง1.1:** Polynomial Curve Fitting Example
* **Bishop, ยง3.1:** Linear Basis Function Models
* **CS229, Lecture Notes #1:** [Part I, Linear Regression](http://cs229.stanford.edu/notes/cs229-notes1.pdf)

Advanced Reading:

* **Blog Post:** Moritz Hardt, ["The Zen of Gradient Descent"](http://blog.mrtz.org/2013/09/07/the-zen-of-gradient-descent.html)
